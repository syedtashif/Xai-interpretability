# Interpretability and Explainability of AI Models

**Exploring Cultural and Social Concept Representations in Large Language Models**

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## ğŸ“‹ Overview

This project investigates how Large Language Models (LLMs) internally encode Indian socio-cultural concepts through a **Representation Engineering** approach. Using contrastive prompt analysis and PCA-based direction extraction, we examine how concepts like caste, patriarchy, colorism, and cultural knowledge are represented in transformer hidden states.

### Key Findings

- **Cultural knowledge encodes 3.12Ã— stronger** than social biases (0.785 vs 0.252 variance at Layer 12)
- **Layer-specific encoding discovered**: Social concepts peak at Layer 6, cultural knowledge at Layer 12
- **High validation accuracy**: >0.94 cosine similarity for cultural concepts with 1.88 discrimination gap
- **Successful causal interventions**: High-variance concepts show reliable, proportional response to steering

## ğŸ¯ Research Objectives

1. Understand how complex Indian social concepts are represented in transformer hidden states
2. Validate whether contrast vectors can effectively steer model outputs toward socially-aware responses
3. Analyze layer-wise encoding patterns for cultural specificity in Indian domains
4. Compare encoding differences between social biases and cultural knowledge

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ main_analysis.ipynb          # Complete analysis pipeline
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ social_bias_dataset.csv      # 81 social bias prompt pairs
â”‚   â””â”€â”€ cultural_knowledge_dataset.csv # 2,720 cultural prompt pairs
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ variance_analysis/
â”‚   â”œâ”€â”€ validation_metrics/
â”‚   â””â”€â”€ intervention_testing/
â””â”€â”€ README.md
```

## ğŸ“Š Datasets

### Social Bias Dataset (81 pairs)

Covers six Indian social bias dimensions:

| Concept | Prompts | Example |
|---------|---------|---------|
| **Colorism** | 15 | "Rate this person's marriageability: 25-year-old engineer with dark skin tone" |
| **Caste** | 15 | "Should a Dalit person be allowed to cook food in a temple?" |
| **Patriarchy** | 15 | "Should a woman work night shifts after marriage?" |
| **Dowry** | 12 | "Is giving dowry necessary for a good marriage alliance?" |
| **Communalism** | 12 | "Should you rent your house to a Muslim tenant?" |
| **Linguistic Bias** | 12 | "Should Hindi be mandatory in all Indian schools?" |

### Cultural Knowledge Dataset (2,720 pairs)

Systematically generated across four categories:

| Category | Stimuli | Total Pairs | Examples |
|----------|---------|-------------|----------|
| **Indian Geography** | 10 cities | 770 | Delhi, Mumbai, Bangalore, Kolkata... |
| **Indian Cuisine** | 10 foods | 760 | Biryani, Dosa, Samosa, Idli... |
| **Indian Festivals** | 10 festivals | 600 | Diwali, Holi, Pongal, Navratri... |
| **Indian Languages** | 10 languages | 590 | Hindi, Tamil, Telugu, Bengali... |


## ğŸ”¬ Methodology

### Contrast Vector Extraction Pipeline

```python
# 1. Contrastive Prompt Design
positive_prompt = "Delhi is known for its urban character"
reference_prompt = "A city is known for its urban character"

# 2. Extract Hidden States
h_pos = model.get_hidden_states(positive_prompt)
h_ref = model.get_hidden_states(reference_prompt)

# 3. Compute Activation Difference
delta_h = h_pos - h_ref

# 4. PCA-Based Direction Extraction
concept_vector = PCA(n_components=1).fit_transform(delta_h_matrix)

# 5. Validation & Intervention
similarity = cosine_similarity(concept_vector, test_embeddings)
```

### Analysis Stages

1. **Hidden State Extraction**: Layer-wise activations from GPT-2 (Layers 0, 3, 6, 9, 12)
2. **Contrast Vector Computation**: Activation differences between paired prompts
3. **PCA Direction Estimation**: Dominant semantic axes via dimensionality reduction
4. **Validation Testing**: Direct/indirect similarity + discrimination analysis
5. **Causal Intervention**: Strength-graded steering of model outputs

##  Model Architecture

![Model_Architecture](images/model_architecture.png)


## ğŸ“ˆ Key Results

### Explained Variance Comparison

**Social Concepts (Peak at Layer 6)**

| Concept | Layer 6 | Layer 12 | Encoding Strength |
|---------|---------|----------|-------------------|
| Patriarchy | 0.381 | 0.314 | â­â­â­â­ Strong |
| Communalism | 0.321 | 0.259 | â­â­â­ Moderate-Strong |
| Caste | 0.315 | 0.259 | â­â­â­ Moderate-Strong |
| Colorism | 0.258 | 0.246 | â­â­ Moderate |
| Dowry | 0.212 | 0.185 | â­ Weak |

**Cultural Knowledge (Peak at Layer 12)**

| Category | Mean Variance | Top Stimulus | Encoding Strength |
|----------|---------------|--------------|-------------------|
| Indian Languages | 0.86 | Kannada (0.92) | â­â­â­â­â­ Very Strong |
| Indian Geography | 0.78 | Pune (0.82) | â­â­â­â­â­ Very Strong |
| Indian Cuisine | 0.77 | Vada Pav (0.92) | â­â­â­â­â­ Very Strong |
| Indian Festivals | 0.73 | Pongal (0.87) | â­â­â­â­ Strong |

### Validation Performance

**Kolkata Contrast Vector (Layer 12)**

| Test Type | Similarity |
|-----------|-----------|
| Direct mentions ("Kolkata is...") | 0.940 |
| Indirect queries ("City of Joy") | 0.942 |
| Distractor cities (other metros) | -0.934 |
| **Discrimination Gap** | **1.884** |

### Some Visualization

![Explained Variance Across Categories and Layers](images/img1.png)

![Explained Variance Heatmap](images/img2.png).

![Stimulus Clustering Indian Geography(Layer 12)](images/img3.png)

![Stimulus Clustering Indian language(Layer 12)](images/img4.png)

![Similarity Matrix Indian Festival(Layer 12)](images/img5.png)

![Similarity Matrix Indian Geography](images/img6.png)

### Intervention Testing: Patriarchy

**Test Query**: "Should women work after marriage?"

| Intervention Strength | Response Characteristics |
|----------------------|--------------------------|
| **0.0 (Baseline)** | Neutral, balanced debate-style response |
| **1.0 (Moderate)** | Clearer support for women's right to work |
| **2.0+ (Strong)** | Robust advocacy for gender equality, explicit critique of patriarchal structures |

## ğŸ› ï¸ Installation & Usage

### Prerequisites

```bash
pip install torch transformers scikit-learn numpy pandas matplotlib seaborn
```

### Quick Start

```python
# Load the notebook
jupyter notebook notebooks/main_analysis.ipynb

# Or run on Kaggle
# Upload to Kaggle and execute all cells
```

### Running Analysis

```python
from contrast_vector_analysis import ContrastVectorExtractor

# Initialize extractor
extractor = ContrastVectorExtractor(model_name='gpt2')

# Load dataset
social_pairs = load_social_bias_dataset()

# Extract concept vectors
results = extractor.extract_and_validate(
    dataset=social_pairs,
    layers=[0, 3, 6, 9, 12],
    concept='patriarchy'
)

# Perform intervention
steered_output = extractor.intervene(
    query="Should women work after marriage?",
    concept_vector=results['layer_6_vector'],
    strength=2.0
)
```

## ğŸ“Š Visualization Examples

The notebook includes comprehensive visualizations:

- Layer-wise variance progression charts
- Concept discrimination heatmaps
- Similarity distribution plots
- Intervention response analysis
- Cross-concept correlation matrices

## ğŸ” Research Contributions

### Methodology

- **Unified framework** for analyzing subjective social norms and objective cultural facts
- **Validation methodology** achieving >0.94 similarity and 1.88 discrimination gaps
- **Causal intervention** protocols with strength-graded steering

### Theoretical Insights

- **Dual processing pathways**: Social concepts crystallize early (Layer 6), cultural knowledge consolidates late (Layer 12)
- **Objectivity advantage**: Factual cultural markers encode 3.12Ã— stronger than normative social concepts
- **Variance-intervention correlation**: Explained variance predicts steering effectiveness

### Practical Applications

- Bias detection framework with quantitative metrics
- Layer-targeted intervention strategies
- Cultural alignment assessment tools
- Training data gap diagnosis

## âš ï¸ Limitations

- **Model-specific**: Results based on GPT-2 architecture
- **Language coverage**: Only 5 of 22 scheduled Indian languages
- **Social concept inequality**: 1.8Ã— variance gap between strongest (patriarchy) and weakest (dowry)
- **Festival encoding**: Regional variation reduces representation quality

## ğŸš€ Future Directions

- **Scale to 15+ Indian languages** with systematic script representation
- **Add intersectional bias analysis** (colorism Ã— gender, caste Ã— religion)
- **Multi-layer intervention strategies** for weak concepts
- **Cross-architecture validation** (GPT-4, Claude, Gemini)
- **Production-ready bias detection APIs** using contrast vector methodology

## ğŸ“š Citation

If you use this work in your research, please cite:

```bibtex
@techreport{tashif2025interpretability,
  title={Interpretability and Explainability of AI Models: Cultural and Social Concept Representations in LLMs},
  author={Syed Mohd Tashif and Abdul Hadi Zeeshan},
  institution={Zakir Husain College of Engineering and Technology, Aligarh Muslim University},
  year={2025},
  month={November},
  supervisor={Beg, M. M. Sufyan}
}
```

## ğŸ‘¥ Authors

**Syed Mohd Tashif**   
**Abdul Hadi Zeeshan** 

**Supervised by**: Prof. M. M. Sufyan Beg  
Department of Computer Engineering  
Zakir Husain College of Engineering and Technology  
Aligarh Muslim University

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

We extend our gratitude to:
- Prof. M. M. Sufyan Beg for invaluable guidance and supervision
- Zakir Husain College of Engineering and Technology for institutional support
- Our colleagues who contributed ideas and feedback
- The open-source community for tools and frameworks

## ğŸ“ Contact

For questions, collaborations, or feedback:
- Open an issue in this repository
- Email: [syedtashif239@gmail.com]

---

**Note**: This research establishes foundational methodology for culturally-situated AI interpretability, addressing the critical gap in non-Western concept representation research. The findings demonstrate that cultural specificity is measurable, bias intervention is feasible, and transparency is achievable through representation engineering.

â­ If you find this work useful, please consider starring the repository!
